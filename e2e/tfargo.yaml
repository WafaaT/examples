apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: tf-workflow-
spec:
  entrypoint: tests
  #onExit: exit-handler
  # Parameters can be passed/overridden via the argo CLI.
  # To override the printed message, run `argo submit` with the -p option:
  # $ argo submit examples/arguments-parameters.yaml -p message="goodbye world"
  arguments:
    parameters:
    - name: datasets-image
      # default image
      value: gcr.io/constant-cubist-173123/tf_workflow:nan_workflow
    - name: aws-access-key-id
      value: ""
    - name: aws-secret-access-key
      value: ""
    - name: tf-master # number of tf masters
      value: 1
    - name: tf-worker # number of tf workers
      value: 1
    - name: tf-ps # number of tf parameter servers
      value: 2
    - name: tf-image
      value: elsonrodriguez/mytfmodel:1.45
    - name: tf-server-image
      value: elsonrodriguez/mytfserver:1.6
    - name: job-name
      value: job21
  volumes:
  - name: training-data
    emptyDir: {}
  - name: training-output
  templates:
  - name: tests
    steps:
      - - name: mount-datasets
          template: kvc
      - - name: get-volume
          template: get-volumemanager-info
      - - name: train
          template: tf-train
          arguments:
            parameters:
            - name: nodeaffinity
              value: "{{steps.get-volume.outputs.parameters.nodeaffinity}}"
            - name: hostpath
              value: "{{steps.get-volume.outputs.parameters.hostpath}}"
      - - name: inference
          template: tf-inference
    # - - name: prepare-datasets
    #     template: download-and-convert-data
  # - name: download-and-convert-data
  #   container:
  #     image: gcr.io/constant-cubist-173123/tf_workflow:nan_workflow
  #     imagePullPolicy: Always
  #     command: ["bash", "-c",  "python /notebooks/models/research/slim/download_and_convert_data.py --dataset_name=flowers --dataset_dir=/tmp/data"]
  #     outputs:
  #       artifacts:
  #       - name: training-data
  #         path: /tmp/data
  - name: kvc
    resource:
      action: apply
      successCondition: status.state == Running
      manifest: |
        apiVersion: aipg.intel.com/v1
        kind: VolumeManager
        metadata:
          name: mnist
          namespace: argo
        spec:
          volumeConfigs:
            - id: "mnist-{{workflow.uid}}"
              replicas: 1
              sourceType: "S3"
              sourceURL: "s3://tfoperator/data/mnist/"
              AccessMode: "ReadWriteOnce"
              Capacity: 1Gi
              Labels:
                tfdata: mnist
              Options:
                awsAccessKeyID: {{workflow.parameters.aws-access-key-id}}
                awsAccessKey: {{workflow.parameters.aws-secret-access-key}}
  - name: get-volumemanager-info
    container:
      image: nervana/circleci:master
      imagePullPolicy: Always
      command: ["bash", "-c", "kubectl get volumemanager mnist -o json | jq -r '.status.volumes[].volumeSource.hostPath.path' | tee /tmp/hostpath; kubectl get volumemanager mnist -o json | jq '.status.volumes[].nodeAffinity' | tee /tmp/nodeaffinity"]
    outputs:
      parameters:
      - name: hostpath
        valueFrom:
          path: /tmp/hostpath
      - name: nodeaffinity
        valueFrom:
          path: /tmp/nodeaffinity
  - name: tf-train
    inputs:
      parameters:
      - name: nodeaffinity
      - name: hostpath
    resource:
      action: apply
      # NOTE: need to detect master node complete
      successCondition: status.state == Succeeded
      manifest: |
        apiVersion: "kubeflow.org/v1alpha1"
        kind: "TFJob"
        metadata:
          name: {{workflow.parameters.job-name}}
        spec:
          replicaSpecs:
            - replicas: {{workflow.parameters.tf-master}}
              tfReplicaType: MASTER
              template:
                spec:
                  affinity:
                    nodeAffinity:
                      {{inputs.parameters.nodeaffinity}}
                  serviceAccountName: tf-job-operator
                  containers:
                    - image: {{workflow.parameters.tf-image}}
                      name: tensorflow
                      imagePullPolicy: Always
                      args: ["python", "/opt/model.py", "--data_dir=/tmp/data", "--train_dir=/tmp/train", "--download=true", "--sync_replicas=true"]
                      volumeMounts:
                      - name: training-result
                        mountPath: /tmp/train
                      - name: training-data
                        mountPath: /tmp/data
                    - image: nervana/circleci:jose_wait_for_master
                      name: upload
                      env:
                      - name: POD_NAME
                        valueFrom:
                          fieldRef:
                            apiVersion: v1
                            fieldPath: metadata.name
                      - name: POD_NAMESPACE
                        valueFrom:
                          fieldRef:
                            apiVersion: v1
                            fieldPath: metadata.namespace
                      command: ['sh', '-c', "./bin/wait_for_master $POD_NAMESPACE $POD_NAME; ls /tmp/train"]
                      volumeMounts:
                      - name: training-result
                        mountPath: /tmp/train
                  volumes:
                  - name: training-result
                    emptyDir: {}
                  - name: training-data
                    hostPath:
                      path: {{inputs.parameters.hostpath}}
                  restartPolicy: OnFailure
            - replicas: {{workflow.parameters.tf-worker}}
              tfReplicaType: WORKER
              template:
                spec:
                  affinity:
                    nodeAffinity:
                      {{inputs.parameters.nodeaffinity}}
                  serviceAccountName: tf-job-operator
                  containers:
                    - image: {{workflow.parameters.tf-server-image}}
                      name: tensorflow
                      imagePullPolicy: Always
                      volumeMounts:
                      - name: training-result
                        mountPath: /tmp/train
                      - name: training-data
                        mountPath: /tmp/data
                    - image: nervana/circleci:jose_wait_for_master
                      name: upload
                      env:
                      - name: POD_NAME
                        valueFrom:
                          fieldRef:
                            apiVersion: v1
                            fieldPath: metadata.name
                      - name: POD_NAMESPACE
                        valueFrom:
                          fieldRef:
                            apiVersion: v1
                            fieldPath: metadata.namespace
                      command: ['sh', '-c', "./bin/wait_for_master $POD_NAMESPACE $POD_NAME; ls /tmp/train"]
                      volumeMounts:
                      - name: training-result
                        mountPath: /tmp/train
                  volumes:
                  - name: training-result
                    emptyDir: {}
                  - name: training-data
                    hostPath:
                      path: {{inputs.parameters.hostpath}}
                  restartPolicy: OnFailure
            - replicas: {{workflow.parameters.tf-ps}}
              tfReplicaType: PS
              template:
                spec:
                  containers:
                    - image: {{workflow.parameters.tf-server-image}}
                      name: tensorflow
                      imagePullPolicy: Always
                  restartPolicy: OnFailure
  - name: tf-inference
    script:
      image: elsonrodriguez/ksonnet:0.8.0-test3
      command: [/ksonnet-entrypoint.sh]
      source: |
        MODEL_COMPONENT=serveMnist
        MODEL_PATH=gs://superpuke/mnist
        MODEL_SERVER_IMAGE=gcr.io/constant-cubist-173123/model-server:1.0

        ks init my-model-server
        cd my-model-server
        ks registry add kubeflow github.com/kubeflow/kubeflow/tree/master/kubeflow
        ks pkg install kubeflow/tf-serving
        ks env add  cloud
        ks generate tf-serving ${MODEL_COMPONENT} --name={{workflow.parameters.job-name}} --namespace=argo --model_path=${MODEL_PATH} --model_server_image=${MODEL_SERVER_IMAGE}
        ks apply cloud -c ${MODEL_COMPONENT}

# NOTE: export mnodel
#   - name: tf_model_export
#                 spec:
#                   containers:
#                     - image: elsonrodriguez/mytfmodel:1.45
#                       name: tensorflow
#     command: python export.py  --checkpoint_dir=/combined_training_results/ --output_dir=/saved_model
